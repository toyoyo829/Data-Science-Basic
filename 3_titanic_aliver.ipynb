{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# titanicデータで線形回帰を行う\n",
    "目的：線形回帰で特徴量からタイタニックの生存予測を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読込\n",
    "#breast_cancer.csvを置いたディレクトリ\n",
    "#\"Sex\"の\"male\"を0、\"female\"を1に変換\n",
    "#\"Embarked\"のSを0、Cを1、Qを2に変換\n",
    "train = pd.read_csv(r\"C:\\Users\\yktft\\python\\M3演習\\M3演習titanic.csv\", encoding=\"utf-8\").replace(\"male\",0).replace(\"female\",1).replace(\"S\",0).replace(\"C\",1).replace(\"Q\",2)\n",
    "test = pd.read_csv(r\"C:\\Users\\yktft\\python\\M3演習\\M3演習titanic_test.csv\", encoding=\"utf-8\").replace(\"male\",0).replace(\"female\",1).replace(\"S\",0).replace(\"C\",1).replace(\"Q\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Age\"の欠損地を平均値に変換\n",
    "train[\"Age\"].fillna(train.Age.median(), inplace=True)\n",
    "#\"Embarked\"の欠損地を平均値に変換\n",
    "train[\"Embarked\"].fillna(train.Embarked.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine1 = [train]\n",
    "\n",
    "#\"Name\"の処理\n",
    "for train in combine1: \n",
    "        train['Salutation'] = train.Name.str.extract(' ([A-Za-z]+).', expand=False) \n",
    "for train in combine1: \n",
    "        train['Salutation'] = train['Salutation'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "        train['Salutation'] = train['Salutation'].replace('Mlle', 'Miss')\n",
    "        train['Salutation'] = train['Salutation'].replace('Ms', 'Miss')\n",
    "        train['Salutation'] = train['Salutation'].replace('Mme', 'Mrs')\n",
    "        del train['Name']\n",
    "Salutation_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} \n",
    "for train in combine1: \n",
    "        train['Salutation'] = train['Salutation'].map(Salutation_mapping) \n",
    "        train['Salutation'] = train['Salutation'].fillna(0)\n",
    "\n",
    "#\"Ticket\"の処理\n",
    "for train in combine1: \n",
    "        train['Ticket_Lett'] = train['Ticket'].apply(lambda x: str(x)[0])\n",
    "        train['Ticket_Lett'] = train['Ticket_Lett'].apply(lambda x: str(x)) \n",
    "        train['Ticket_Lett'] = np.where((train['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), train['Ticket_Lett'], np.where((train['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0')) \n",
    "        train['Ticket_Len'] = train['Ticket'].apply(lambda x: len(x)) \n",
    "        del train['Ticket'] \n",
    "train['Ticket_Lett']=train['Ticket_Lett'].replace(\"1\",1).replace(\"2\",2).replace(\"3\",3).replace(\"0\",0).replace(\"S\",3).replace(\"P\",0).replace(\"C\",3).replace(\"A\",3)\n",
    "\n",
    "#\"Cabin\"の処理\n",
    "for train in combine1: \n",
    "    train['Cabin_Lett'] = train['Cabin'].apply(lambda x: str(x)[0]) \n",
    "    train['Cabin_Lett'] = train['Cabin_Lett'].apply(lambda x: str(x)) \n",
    "    train['Cabin_Lett'] = np.where((train['Cabin_Lett']).isin([ 'F', 'E', 'D', 'C', 'B', 'A']),train['Cabin_Lett'], np.where((train['Cabin_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']), '0','0'))\n",
    "del train['Cabin'] \n",
    "train['Cabin_Lett']=train['Cabin_Lett'].replace(\"A\",1).replace(\"B\",2).replace(\"C\",1).replace(\"0\",0).replace(\"D\",2).replace(\"E\",2).replace(\"F\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FamilySizeを追加(Sibsp+Parch+1)\n",
    "#FamilySizeから、IsAlone追加\n",
    "train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n",
    "for train in combine1:\n",
    "    train['IsAlone'] = 0\n",
    "\n",
    "    train.loc[train['FamilySize'] == 1, 'IsAlone'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   0.   3. ...   0.   2.   0.]\n",
      " [  2.   1.   1. ...   1.   2.   0.]\n",
      " [  3.   1.   3. ...   0.   1.   1.]\n",
      " ...\n",
      " [889.   0.   3. ...   0.   4.   0.]\n",
      " [890.   1.   1. ...   1.   1.   1.]\n",
      " [891.   0.   3. ...   0.   1.   1.]]\n",
      "[[ 3.  0. 22. ...  0.  2.  0.]\n",
      " [ 1.  1. 38. ...  1.  2.  0.]\n",
      " [ 3.  1. 26. ...  0.  1.  1.]\n",
      " ...\n",
      " [ 3.  1. 28. ...  0.  4.  0.]\n",
      " [ 1.  0. 26. ...  1.  1.  1.]\n",
      " [ 3.  0. 32. ...  0.  1.  1.]]\n",
      "[0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "#正解データと学習用のデータに分ける\n",
    "train_data = train.values\n",
    "xs_train = train_data[:, 2:] # Pclass以降の変数\n",
    "y_train  = train_data[:, 1]  # 正解データ\n",
    "print(train_data)\n",
    "print(xs_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testもtrainと同様に前処理実施\n",
    "test[\"Age\"].fillna(train.Age.mean(), inplace=True)\n",
    "test[\"Fare\"].fillna(train.Fare.mean(), inplace=True)\n",
    "\n",
    "combine2 = [test]\n",
    "for test in combine2:\n",
    "    test['Salutation'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "for test in combine2:\n",
    "    test['Salutation'] = test['Salutation'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    "         'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    test['Salutation'] = test['Salutation'].replace('Mlle', 'Miss')\n",
    "    test['Salutation'] = test['Salutation'].replace('Ms', 'Miss')\n",
    "    test['Salutation'] = test['Salutation'].replace('Mme', 'Mrs')\n",
    "    del test['Name']\n",
    "Salutation_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "for test in combine2:\n",
    "    test['Salutation'] = test['Salutation'].map(Salutation_mapping)\n",
    "    test['Salutation'] = test['Salutation'].fillna(0)\n",
    "\n",
    "for test in combine2:\n",
    "        test['Ticket_Lett'] = test['Ticket'].apply(lambda x: str(x)[0])\n",
    "        test['Ticket_Lett'] = test['Ticket_Lett'].apply(lambda x: str(x))\n",
    "        test['Ticket_Lett'] = np.where((test['Ticket_Lett']).isin(['1', '2', '3', 'S', 'P', 'C', 'A']), test['Ticket_Lett'],\n",
    "                                   np.where((test['Ticket_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n",
    "                                            '0', '0'))\n",
    "        test['Ticket_Len'] = test['Ticket'].apply(lambda x: len(x))\n",
    "        del test['Ticket']\n",
    "test['Ticket_Lett']=test['Ticket_Lett'].replace(\"1\",1).replace(\"2\",2).replace(\"3\",3).replace(\"0\",0).replace(\"S\",3).replace(\"P\",0).replace(\"C\",3).replace(\"A\",3) \n",
    "\n",
    "for test in combine2:\n",
    "        test['Cabin_Lett'] = test['Cabin'].apply(lambda x: str(x)[0])\n",
    "        test['Cabin_Lett'] = test['Cabin_Lett'].apply(lambda x: str(x))\n",
    "        test['Cabin_Lett'] = np.where((test['Cabin_Lett']).isin(['T', 'H', 'G', 'F', 'E', 'D', 'C', 'B', 'A']),test['Cabin_Lett'],\n",
    "                                   np.where((test['Cabin_Lett']).isin(['W', '4', '7', '6', 'L', '5', '8']),\n",
    "                                            '0','0'))        \n",
    "        del test['Cabin']\n",
    "test['Cabin_Lett']=test['Cabin_Lett'].replace(\"A\",1).replace(\"B\",2).replace(\"C\",1).replace(\"0\",0).replace(\"D\",2).replace(\"E\",2).replace(\"F\",1).replace(\"G\",1) \n",
    "\n",
    "test[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n",
    "\n",
    "for test in combine2:\n",
    "    test['IsAlone'] = 0\n",
    "    test.loc[test['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "    \n",
    "test_data = test.values\n",
    "xs_test = test_data[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifierで学習\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest=RandomForestClassifier()\n",
    "random_forest.fit(xs_train, y_train)\n",
    "Y_pred = random_forest.predict(xs_test)\n",
    "\n",
    "#predict_result_dataに出力\n",
    "import csv\n",
    "with open(\"predict_result_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    writer.writerow([\"PassengerId\", \"Survived\"])\n",
    "    for pid, survived in zip(test_data[:,0].astype(int), Y_pred.astype(int)):\n",
    "        writer.writerow([pid, survived])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
